{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d366684",
   "metadata": {},
   "source": [
    "##### Next Word Predictor Project using LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10400d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "with open('cricket_data.txt', 'r') as file:\n",
    "    data = file.read() # Reading the entire file\n",
    "\n",
    "data = data.lower().replace('\\n', ' ').replace('  ', ' ').strip()\n",
    "print(data) # Our cricket data is loaded to the data variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccffc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras as kr\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9773d66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token = '<oov>')\n",
    "tokenizer.fit_on_texts([data]) # Fitting the tokenizer on our textual data. Note, we passed our data in the form of a list because there can be multiple texts. So we need to pass them in the form of a list\n",
    "print('Word Indices:', tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6b4199",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = list()\n",
    "for sentence in data.split('.'):\n",
    "    # print(sentence, end = ' | ') # To see the sentences in our data\n",
    "    tokenized_sent = tokenizer.texts_to_sequences([sentence])[0]\n",
    "    # print(tokenized_sent, end = ' | ') # To see the tokenized sentence\n",
    "    for i in range(1, len(tokenized_sent)):\n",
    "        input_sequences.append(tokenized_sent[:i+1]) # From the starting till i (i+1 is excluded as per rule)\n",
    "\n",
    "print(input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c902594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to adjust all the input sequences to a same dimension, we need to find the max length sentence and apply zero padding\n",
    "length = [len(x) for x in input_sequences]\n",
    "max_len = max(length)\n",
    "print('Max length:', max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abaa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import pad_sequences\n",
    "padded_input_sequences = pad_sequences(sequences = input_sequences, maxlen = max_len, padding = 'pre') # We applied padding at the starting because we want to create an input output behaviour. We want to keep the output at the end of sequence\n",
    "padded_input_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bc8c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to create a input output behaviour out of the sequences, as we can see that, the last number in the sequence is the output and the rest all of them are the input..\n",
    "X = padded_input_sequences[:, :-1] # All rows and all columns excluding the last (-1) indexed col\n",
    "y = padded_input_sequences[:, -1] # All rows and only the -1 column\n",
    "print(X, '\\n\\n',  y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b8529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5227e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need our vocabulary size for the num_classes and input_dimension\n",
    "vocab_size = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e91e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will apply One Hot Encoding on our data\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y, num_classes = vocab_size + 1) # We added an extra 1 because in tokenizer, words are tokenized starting from 1, and OHE always starts from index 0. So if we do not give the extra 1, the last word will always be missed\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b50118c",
   "metadata": {},
   "source": [
    "##### Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15090601",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = vocab_size + 1, output_dim = 100, input_shape = (max_len,))) # Output_dimension is our choice. It is a hyperparameter which we can tune to find out the best results. Input_shape is the size of each input in our input sequence. Embedding layer does a very simple job. It takes each one of the unique word in our vocabulary and converts it into a 100 (since here our output_dim is 100) dimension numeric vector.\n",
    "model.add(LSTM(units = 150))\n",
    "model.add(Dense(units = vocab_size + 1, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae59ef",
   "metadata": {},
   "source": [
    "##### Compilation and Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7590c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = kr.optimizers.Adam(), loss = kr.losses.categorical_crossentropy, metrics = ['accuracy'])\n",
    "history = model.fit(X, y, epochs = 100, verbose = 1, validation_split = 0.1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8fd00",
   "metadata": {},
   "source": [
    "##### Prediction of the next 'n' words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685e81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(input('How many words you want to be predicted? '))\n",
    "new_word = input('Enter a word: ').lower()\n",
    "for _ in range(n):\n",
    "    tokenized_new_word = tokenizer.texts_to_sequences([new_word])[0]\n",
    "    padded_tokenized_new_word = pad_sequences([tokenized_new_word], maxlen = max_len - 1, padding = 'pre') # max_len - 1 for padding is used because our model input (X) was padded_input_sequences[:, :-1]\n",
    "    print(padded_tokenized_new_word)\n",
    "\n",
    "    raw_prob = model.predict(padded_tokenized_new_word) # Returns the raw probability vector\n",
    "    pred = np.argmax(raw_prob) # This returns the highest probability\n",
    "    conf = np.max(raw_prob)\n",
    "    for key, val in tokenizer.word_index.items():\n",
    "        if val == pred:\n",
    "            # print(f'Predicted word is \"{key}\" with a confidence of {conf:.4f}.')\n",
    "            new_word = new_word + \" \" + key\n",
    "            print(new_word)\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
